\documentclass[journal, a4paper]{IEEEtran}

% some very useful LaTeX packages include:

%\usepackage{cite}      % Written by Donald Arseneau
                        % V1.6 and later of IEEEtran pre-defines the format
                        % of the cite.sty package \cite{} output to follow
                        % that of IEEE. Loading the cite package will
                        % result in citation numbers being automatically
                        % sorted and properly "ranged". i.e.,
                        % [1], [9], [2], [7], [5], [6]
                        % (without using cite.sty)
                        % will become:
                        % [1], [2], [5]--[7], [9] (using cite.sty)
                        % cite.sty's \cite will automatically add leading
                        % space, if needed. Use cite.sty's noadjust option
                        % (cite.sty V3.8 and later) if you want to turn this
                        % off. cite.sty is already installed on most LaTeX
                        % systems. The latest version can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/cite/

\usepackage{graphicx}   % Written by David Carlisle and Sebastian Rahtz
                        % Required if you want graphics, photos, etc.
                        % graphicx.sty is already installed on most LaTeX
                        % systems. The latest version and documentation can
                        % be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/required/graphics/
                        % Another good source of documentation is "Using
                        % Imported Graphics in LaTeX2e" by Keith Reckdahl
                        % which can be found as esplatex.ps and epslatex.pdf
                        % at: http://www.ctan.org/tex-archive/info/

%\usepackage{psfrag}    % Written by Craig Barratt, Michael C. Grant,
                        % and David Carlisle
                        % This package allows you to substitute LaTeX
                        % commands for text in imported EPS graphic files.
                        % In this way, LaTeX symbols can be placed into
                        % graphics that have been generated by other
                        % applications. You must use latex->dvips->ps2pdf
                        % workflow (not direct pdf output from pdflatex) if
                        % you wish to use this capability because it works
                        % via some PostScript tricks. Alternatively, the
                        % graphics could be processed as separate files via
                        % psfrag and dvips, then converted to PDF for
                        % inclusion in the main file which uses pdflatex.
                        % Docs are in "The PSfrag System" by Michael C. Grant
                        % and David Carlisle. There is also some information
                        % about using psfrag in "Using Imported Graphics in
                        % LaTeX2e" by Keith Reckdahl which documents the
                        % graphicx package (see above). The psfrag package
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/psfrag/

%\usepackage{subfigure} % Written by Steven Douglas Cochran
                        % This package makes it easy to put subfigures
                        % in your figures. i.e., "figure 1a and 1b"
                        % Docs are in "Using Imported Graphics in LaTeX2e"
                        % by Keith Reckdahl which also documents the graphicx
                        % package (see above). subfigure.sty is already
                        % installed on most LaTeX systems. The latest version
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/subfigure/

\usepackage{url}        % Written by Donald Arseneau
                        % Provides better support for handling and breaking
                        % URLs. url.sty is already installed on most LaTeX
                        % systems. The latest version can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/other/misc/
                        % Read the url.sty source comments for usage information.

%\usepackage{stfloats}  % Written by Sigitas Tolusis
                        % Gives LaTeX2e the ability to do double column
                        % floats at the bottom of the page as well as the top.
                        % (e.g., "\begin{figure*}[!b]" is not normally
                        % possible in LaTeX2e). This is an invasive package
                        % which rewrites many portions of the LaTeX2e output
                        % routines. It may not work with other packages that
                        % modify the LaTeX2e output routine and/or with other
                        % versions of LaTeX. The latest version and
                        % documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/sttools/
                        % Documentation is contained in the stfloats.sty
                        % comments as well as in the presfull.pdf file.
                        % Do not use the stfloats baselinefloat ability as
                        % IEEE does not allow \baselineskip to stretch.
                        % Authors submitting work to the IEEE should note
                        % that IEEE rarely uses double column equations and
                        % that authors should try to avoid such use.
                        % Do not be tempted to use the cuted.sty or
                        % midfloat.sty package (by the same author) as IEEE
                        % does not format its papers in such ways.

\usepackage{amsmath}    % From the American Mathematical Society
                        % A popular package that provides many helpful commands
                        % for dealing with mathematics. Note that the AMSmath
                        % package sets \interdisplaylinepenalty to 10000 thus
                        % preventing page breaks from occurring within multiline
                        % equations. Use:
%\interdisplaylinepenalty=2500
                        % after loading amsmath to restore such page breaks
                        % as IEEEtran.cls normally does. amsmath.sty is already
                        % installed on most LaTeX systems. The latest version
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
%   keywordstyle=\color{blue},
%   commentstyle=\color{dkgreen},
%   stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

% Other popular packages for formatting tables and equations include:

%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty which improves the
% LaTeX2e array and tabular environments to provide better appearances and
% additional user controls. array.sty is already installed on most systems.
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/

% V1.6 of IEEEtran contains the IEEEeqnarray family of commands that can
% be used to generate multiline equations as well as matrices, tables, etc.

% Also of notable interest:
% Scott Pakin's eqparbox package for creating (automatically sized) equal
% width boxes. Available:
% http://www.ctan.org/tex-archive/macros/latex/contrib/supported/eqparbox/

% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.


% Your document starts here!
\begin{document}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 %----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

~\\[1cm]
\includegraphics{SCUT.png}\\[2cm] % Include a department/university logo - this will require the graphicx package

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[1cm]
{ \huge \bfseries The Experiment Report of \textit{Machine Learning} }\\[0.6cm] % Title of your document
\HRule \\[2cm]
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------


\textsc{\LARGE \textbf{School:} School of Software Engineering}\\[1cm]
\textsc{\LARGE \textbf{Subject:} Software Engineering}\\[2cm] 

 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Jining He % Your name
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Qingyao Wu % Supervisor's Name
\end{flushright}
\end{minipage}\\[2cm]
~
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Student ID:}\\
201721045565
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Grade:} \\
Graduate
\end{flushright}
\end{minipage}\\[2cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

% Define document title and author
	\title{Logistic Regression, Linear Classification and Stochastic Gradient Descent}
	\maketitle

% Write abstract here
\begin{abstract}
    In this experiment, we use stochastic gradient descent and other optimization algorithms to optimize logistic regression and linear classification on larger dataset. Adam optimization algorithm has the best performance.
% The short abstract is intended to give the reader an overview of the experiment. It should be brief and to the point. 
\end{abstract}

% Each section begins with a \section{title} command
\section{Introduction}
	% \PARstart{}{} creates a tall first letter for this first paragraph
% \PARstart{T}{his} section introduces the problem to solved and leads the reader on to the main part. Detailed motivation is necessary. What's more, you can show your expected results and contributions.
\PARstart{T}{his} experiment require us to predict adult whether income exceeds \$50K/yr. We will use both logistic regression and linear classification. We also will use four optimization algorithms.

The Motivation of this experiment is let us compare and understand the differences between gradient descent and stochastic gradient descent, compare and understand the differences and relationships between logistic regression and linear classification, further understand the principles of SVM and practice on larger data.


% 对比理解梯度下降和随机梯度下降的区别与联系。
% 对比理解逻辑回归和线性分类的区别与联系。
% 进一步理解SVM的原理并在较大数据上实践。


% Main Part
\section{Methods and Theory}
% In this section, you are asked to give a complete introduction to the experiment. For instance, the chosen methods, the related theories, the related equations(loss function), the derivation process(taking the gradient) and so on.
In this experiment, we use logistic regression and linear classifier to predict adult whether income exceeds \$50K/yr.

\subsection{Logistic Regression}
In logistic regression, the hypothesis function is:
\begin{equation} \label{eq:logistic hypothesis function}
    h_{w,b}(x) = g(w^Tx+b) = \frac{1}{1+e^{-(w^Tx+b)}}
\end{equation}
and the loss function is:
\begin{equation} \label{eq:logistic regression loss function}
    J(W,b) = -\frac{1}{m}\sum_{i=1}^m(y_i\log{h_{W,b}(x_i)}+(1-y_i)\log{(1-h_{W,b}(x_i))}
\end{equation} 
and the gradient formulas are:
\begin{equation} \label{eq:logistic regression gradient formula}
    \begin{aligned}
        \nabla_WJ(W,b)&=\frac{1}{m}\sum_{i=1}^m(h_{W,b}(x_i)-y_i)x_i\\
        \nabla_bJ(W,b)&=\frac{1}{m}\sum_{i=1}^m(h_{W,b}(x_i)-y_i)
    \end{aligned}
\end{equation}

\subsection{Linear Classification}
In linear classification, we add hinge loss to our loss function:
\begin{equation} \label{eq:linear classification loss function}
    J(W,b) = \frac{||W||^2}2+\frac{C}m\sum_{i=1}^{m}max(0,1-y_i(W^Tx_i+b))
\end{equation}
and the gradient formulas are:

\begin{equation} \label{eq:linear classification gradient formula}
    \begin{aligned}
        \begin{aligned}
            g_w(x_i) &= \left\{
                \begin{aligned} 
                    & -y_ix_i & &1-y_i(w^Tx_i+b)>=0 \\
                    & 0 & & 1-y_i(w^Tx_i+b)<0
                \end{aligned}
            \right. \\
            g_b(x_i) &= \left\{
                \begin{aligned} 
                    & -y_i & &1-y_i(w^Tx_i+b)>=0 \\
                    & 0 & & 1-y_i(w^Tx_i+b)<0
                \end{aligned}
            \right. 
        \end{aligned}\\
        \begin{aligned}
            \nabla_WJ(W,b) &= W+\frac{C}m\sum_{i=1}^{m}g_w(x_i)\\
            \nabla_bJ(W,b) &= \frac{C}m\sum_{i=1}^{m}g_b(x_i)
        \end{aligned}
    \end{aligned}
\end{equation}

% \begin{equation} \label{eq:linear classification gradient formula}
%     \begin{aligned}
%         \nabla_WJ(W,b) &= W+\frac{C}m\sum_{i=1}^{m}g_w(x_i)\\
%         \nabla_bJ(W,b) &= \frac{C}m\sum_{i=1}^{m}g_b(x_i)
%     \end{aligned}
% \end{equation}

\subsection{Gradient Descent Optimization Algorithms}
In the last experiment, we used batch gradient descent to optimize model.
In fact, there are many gradient descent optimization algorithms. In this experiment, we use some of them.

\subsubsection{NAG}
Below is the NAG update parameters formula, and momentum term $\gamma$ is around 0.9.
\begin{equation}
    \begin{aligned}
        g_t&= \nabla_\theta J(\theta_{t-1}-\gamma v_{t-1})\\
        v_t&=\gamma v_{t-1}+\alpha g_t\\
        \theta_t &= \theta_{t-1} - v_t
    \end{aligned}
\end{equation}

% \begin{equation}
%     \begin{aligned}
%         g_t&\leftarrow\nabla J(\theta_{t-1}-\gamma v_{t-1})\\
%         v_t&\leftarrow\gamma v_{t-1}+\eta g_t\\
%         \theta_t&\leftarrow\theta_{t-1}-v_t
%     \end{aligned}
% \end{equation}

\subsubsection{RMSProp}
The term $\epsilon$ is usually $10^{-8}$.
\begin{equation}
    \begin{aligned}
        g_t&=\nabla J(\theta_{t-1})\\
        G_t&=\gamma G_t + (1-\gamma)g_t^2\\
        \theta_t&=\theta_{t-1}-\frac{\alpha}{\sqrt{G_t}+\epsilon}g_t
    \end{aligned}
\end{equation}
% \begin{equation}
%     \begin{aligned}
%         g_t&\leftarrow\nabla J(\theta_{t-1})\\
%         G_t&\leftarrow\gamma G_t + (1-r)g_t\odot g_t\\
%         \theta_t&\leftarrow\theta_{t-1}-\frac{\eta}{\sqrt{G_t+e}}\odot g_t
%     \end{aligned}
% \end{equation}
\subsubsection{AdaDelta}
\begin{equation}
    \begin{aligned}
        g_t &= \nabla J(\theta_{t-1})\\
        G_t &= \gamma G_t + (1-\gamma)g_t^2\\
        \theta_t &= \theta_{t-1}-\frac{\alpha}{\sqrt{G_t}+\epsilon}g_t
    \end{aligned}
\end{equation}



\subsubsection{Adam}
The term $\beta_1$ is usually 0.9 and the term $\beta_2$ is 0.999.
\begin{equation}
    \begin{aligned}
        g_t &= \nabla J(\theta_{t-1})\\
        m_t &= \beta_1m_{t-1}+(1-\beta_1)g_t\\
        v_t &= \beta_2v_{t-1}+(1-\beta_2)g_t\\
        \theta_t &= \theta_{t-1}-\frac{\alpha}{\sqrt{G_t}+\epsilon}m_t
    \end{aligned}
\end{equation}

% \begin{equation}G_t\leftarrow\gamma G_t + (1-r)g_t\odot g_t\end{equation}
% \begin{equation}\triangle\theta_t\leftarrow -\frac{\sqrt{\triangle_{t-1}+e}}{\sqrt{G_T+e}}\odot g_t\end{equation}
% \begin{equation}\theta_t\leftarrow\theta_{t-1}+\triangle\theta_t\end{equation}
% \begin{equation}\triangle t\leftarrow\gamma\triangle_{t-1}+(1-\gamma)\triangle\theta_t\odot\triangle\theta_t\end{equation}

% We update both models using:
% \begin{equation} \label{eq:update parameters formula}
%     \begin{aligned}
%         W&=W-\alpha \nabla_WJ(W,b)\\
%         b&=b-\alpha \nabla_bJ(W,b)
%     \end{aligned}
% \end{equation}


% 在本次实验中，我们使用线性回归来预测房屋价格，使用线性分类判断是否为澳大利亚人。
% 在线性回归中，我们使用的Loss Function为
% 梯度为

% 在线性分类中，我们使用SVM分类器，higo loss, LossFunction为
% 梯度为

% This experiment let us learn Linear Regression and Linear Classification. 

% Linear Regression
% loss function
% gradient formula

% Linear Classification
% loss function
% gradient formula


\section{Experiments}
\subsection{Dataset}
The datasets are from LIBSVM. This experiment uses a9a dataset, including 32561/16281(testing) samples and each sample has 123 features.
% This section represents the related information of datasets, such as the content, the number of data, the training set, the validation set and so on.

\subsection{Implementation}
% All detailed implementation in your experiment: initialization, process, results, all kinds of parameters. In a word, describe clearly What you do and how you do.\par

\subsubsection{Data Preprocessing}
Use load\textunderscore svmlight\textunderscore file function in sklearn library to load train set and validation set.
Then convert data to column vector.

\subsubsection{Initialization}
There are many ways to initialize linear model parameters.
I choose to set all parameters into zero.
% 有多种方法初始化参数，在这里，我们使用零初初始化参数W,b。
% Initialize linear model parameters. You can choose to set all parameter into zero, initialize it randomly or with normal distribution.
\subsubsection{Compute Loss and Gradient} \label{compute loss and gradient}
In logistic regression, use formula \eqref{eq:logistic regression loss function} to compute loss and formula \eqref{eq:logistic regression gradient formula} to compute gradient.
In linear classification, use formula \eqref{eq:linear classification loss function} to compute loss and formula \eqref{eq:linear classification gradient formula} to compute gradient.

% 使用公式求loss function 
% 在Linear Regression 我们采用的Loss Function为
% 在LInear Classification 我们的lossfunction为
% Loss function
% \subsubsection{Gradient}
% 使用公式求梯度
% 在Linear Regression 梯度公式为
% 在LInear Classification 梯度公式为
% calulate gradient 



\subsubsection{Update Parameters} \label{update parameters}
The parameters are updated according to the optimization algorithm used.


\subsubsection{Repeat}
Repeat step 3 and 4 until reach the end condition. Get trian loss and validation loss in every iteration.

% 重复上面的步骤，并在在训练集、测试集上计算loss
% get loss L train under training set and validation set
% repeat step \ref{xxxx} to 8

\subsubsection{Implementation Code}

I use a function to random and split dataset to many mini-batches:
\begin{lstlisting}[language=Python]
def random_batches(X,Y,batch_size):
    m = X.shape[1]
    batches = []
    permutation = np.random.permutation(m)
    shuffled_X = X[:,permutation]
    shuffled_Y = Y[:,permutation]
    num_batches = math.floor(m/batch_size)
    for i in range(num_batches):
        batch_X = shuffled_X[:, i*batch_size:(i+1)*batch_size]
        batch_Y = shuffled_Y[:, i*batch_size:(i+1)*batch_size]
        batch = (batch_X, batch_Y)
        batches.append(batch)
    if m % batch_size != 0:
        batch_X = shuffled_X[:, num_batches*batch_size:m]
        batch_Y = shuffled_Y[:, num_batches*batch_size:m]
        batch = (batch_X, batch_Y)
        batches.append(batch)
    return batches
\end{lstlisting}
In logistic regression, I use this function to compute cost and gradient:

\begin{lstlisting}[language=Python]
def propagate(W,b,X,Y):
    m = X.shape[1]
    A = sigmoid(np.dot(W.T,X)+b)
    cost = (np.dot(Y,np.log(A).T) + np.dot((1-Y),np.log(1-A).T))/(-m)
    gW = np.dot(X,(A-Y).T)/m  
    gb = np.sum(A-Y)/m
    cost = np.squeeze(cost)
    return cost,gW,gb
\end{lstlisting}
And in linear classification, I use this function:
\begin{lstlisting}[language=Python]
def propagate(C,W,b,X,Y):
    m = X.shape[1]
    A = np.dot(W.T,X)+b
    cost = np.sum(np.square(W))/2 + C*np.sum(np.maximum(0, 1-Y*(np.dot(W.T,X)+b)))/m
    filt = (1-Y*(np.dot(W.T,X)+b))>0
    gW = W - C*np.dot(Y*filt,X.T).T/m
    gb = -C*np.sum(Y*filt*b)/m
    return cost,gW,gb
\end{lstlisting}

The following is the formula for each optimization algorithm to update the parameters.
NAG:
\begin{lstlisting}[language=Python]
    vgW_prev = vgW
    vgW = beta * vgW - learning_rate * gW
    W = W - beta * vgW_prev + (1+beta) * vgW
    vgb_prev = vgb
    vgb = beta * vgb - learning_rate * gb
    b = b - beta * vgb_prev + (1+beta) * vgb
\end{lstlisting}
RMSProp:

\begin{lstlisting}[language=Python]
    W_cache = decay_rate * W_cache + (1 - decay_rate) * gW**2
    W = W - learning_rate * gW / (np.sqrt(W_cache) + eps)
    b_cache = decay_rate * b_cache + (1 - decay_rate) * gb**2
    b = b - learning_rate * gb / (np.sqrt(b_cache) + eps)
\end{lstlisting}
AdaDelta:

\begin{lstlisting}[language=Python]
    W_cache = beta * W_cache + (1-beta) * gW**2
    delta_W =  - np.sqrt(delta_with_W + eps)/(np.sqrt(W_cache)+eps) * gW
    W = W + delta_W
    delta_with_W = beta * delta_with_W + (1-beta)* delta_W ** 2
    b_cache = beta * b_cache + (1-beta) * gb**2
    delta_b =  - np.sqrt(delta_with_b + eps)/(np.sqrt(b_cache)+eps) * gb
    b = b + delta_b
    delta_with_b = beta * delta_with_b + (1-beta)* delta_b ** 2
\end{lstlisting}
Adam:

\begin{lstlisting}[language=Python]
    vgW = beta1*vgW + (1-beta1)*gW
    sgW = beta2*sgW + (1-beta2)*(gW**2)
    W = W - learning_rate * vgW / (np.sqrt(sgW) + eps)
    vgb = beta1*vgb + (1-beta1)*gb
    sgb = beta2*sgb + (1-beta2)*(gb**2)
    b = b - learning_rate * vgb / (np.sqrt(sgb) + eps)
\end{lstlisting}


\subsubsection{Result}

I plot both logistic regression and linear classification. 
Fig~\ref{fig:logistic_regression_loss_with_different_optimization_algorithm} and Fig~\ref{fig:linear_classification_loss_with_different_optimization_algorithm} show the performance of these optimization algorithms. 
From the convergence speed, all four optimization algorithms are better than MSGD, and the Adam Algorithm is the best. 

Compare Fig~\ref{fig:logistic_regression_loss_with_different_optimization_algorithm} and Fig~\ref{fig:linear_classification_loss_with_different_optimization_algorithm}, we can find that the performance is different with different model. 
In logistic regression, RMSProp is better than NAG. 
However, in linear classification, NAG is better than RMSprop.
Moreover, we can also find that the performance of linear classification is better than logistic regression.


\begin{figure}[!hbt]
    \begin{center}
    \includegraphics[width=\columnwidth]{logistic_regression_loss_with_different_optimization_algorithm}
    \caption{The curves of the logistic regression loss function with different optimization algorithm. $Batch\_size=4096$ and $\alpha=0.01$.}
    \label{fig:logistic_regression_loss_with_different_optimization_algorithm}
    \end{center}
\end{figure}

\begin{figure}[!hbt]
    \begin{center}
    \includegraphics[width=\columnwidth]{linear_classification_loss_with_different_optimization_algorithm}
    \caption{The curves of the linear classification loss function with different optimization algorithm. $Batch\_size=4096$ and $\alpha=0.001$.}
    \label{fig:linear_classification_loss_with_different_optimization_algorithm}
    \end{center}
\end{figure}

I also plot linear classification with different batch size.
From Fig~\ref{fig:linear_classification_loss_with_different_batch_size} we can find that the smaller batch, the greater the loss function shock. So when we train a machine learning model, we need to select an appropriate batch size.

\begin{figure}[!hbt]
    \begin{center}
    \includegraphics[width=\columnwidth]{linear_classification_loss_with_different_batch_size}
    \caption{The curves of the linear classification loss function with different batch size. $\alpha=0.01$}
    \label{fig:linear_classification_loss_with_different_batch_size}
    \end{center}
\end{figure}






% In Linear regression
% Loss shows at Fig.~\ref{fig:linear_regression_loss}
% \begin{figure}[!hbt]
%     % Center the figure.
%     \begin{center}
%     % Include the eps file, scale it such that it's width equals the column width. You can also put width=8cm for example...
%     \includegraphics[width=\columnwidth]{linear_regression_loss}
%     % Create a subtitle for the figure.
%     \caption{Simulation results on the AWGN channel. Average throughput $k/n$ vs $E_s/N_0$.}
%     % Define the label of the figure. It's good to use 'fig:title', so you know that the label belongs to a figure.
%     \label{fig:linear_regression_loss}
%     \end{center}
% \end{figure}



% This is how you include a eps figure in your document. LaTeX only accepts EPS or TIFF files.




	% % You can reference tables and figure by using the \ref{label} command. Each table and figure needs to have a UNIQUE label.
	% Figures and tables should be labeled and numbered, such as in Table~\ref{tab:simParameters} and Fig.~\ref{fig:tf_plot}.

	% % This is how you define a table: the [!hbt] means that LaTeX is forced (by the !) to place the table exactly here (by h), or if that doesnt work because of a pagebreak or so, it tries to place the table to the bottom of the page (by b) or the top (by t).
	% \begin{table}[!hbt]
	% 	% Center the table
	% 	\begin{center}
	% 	% Title of the table
	% 	\caption{Simulation Parameters}
	% 	\label{tab:simParameters}
	% 	% Table itself: here we have two columns which are centered and have lines to the left, right and in the middle: |c|c|
	% 	\begin{tabular}{|c|c|}
	% 		% To create a horizontal line, type \hline
	% 		\hline
	% 		% To end a column type &
	% 		% For a linebreak type \\
	% 		Information message length & $k=16000$ bit \\
	% 		\hline
	% 		Radio segment size & $b=160$ bit \\
	% 		\hline
	% 		Rate of component codes & $R_{cc}=1/3$\\
	% 		\hline
	% 		Polynomial of component encoders & $[1 , 33/37 , 25/37]_8$\\
	% 		\hline
	% 	\end{tabular}
	% 	\end{center}
	% \end{table}

	% % If you have questions about how to write mathematical formulas in LaTeX, please read a LaTeX book or the 'Not So Short Introduction to LaTeX': tobi.oetiker.ch/lshort/lshort.pdf

	% % This is how you include a eps figure in your document. LaTeX only accepts EPS or TIFF files.
	% \begin{figure}[!hbt]
	% 	% Center the figure.
	% 	\begin{center}
	% 	% Include the eps file, scale it such that it's width equals the column width. You can also put width=8cm for example...
	% 	\includegraphics[width=\columnwidth]{plot_tf}
	% 	% Create a subtitle for the figure.
	% 	\caption{Simulation results on the AWGN channel. Average throughput $k/n$ vs $E_s/N_0$.}
	% 	% Define the label of the figure. It's good to use 'fig:title', so you know that the label belongs to a figure.
	% 	\label{fig:tf_plot}
	% 	\end{center}
	% \end{figure}


\section{Conclusion}

	% This section summarizes the paper. In our experiments, you can also write your gains and inspirations in here.
    Through this experiment, I understood the difference between gradient descent and stochastic gradient descent. Stochastic gradient descent uses only a small size of data for training, which accelerates the training process. However, if the data size of each batch is too small, there will be a big shock, or even can not converge.
    I also found that SVM as a classifier works better than logistic regression.
    At the same time, I implemented four gradient descent optimization algorithms that make me understand gradient descent better.


% Your document ends here!
\end{document}